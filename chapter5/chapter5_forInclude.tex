\chapter{Statistics: what it is and why it works}

\section{Motivation}

We have by now learned a whole lot about probability theory. In the previous chapter we have seen
how to compute a distribution of a RV given another RV. Moreover, we know how to factor joint distributions and simplify them by making
independence assumptions. In principle, this puts us in a good position to start formulating our
own probabilistic models. However, our models will be pretty useless if we do not know their parameters. And as
it so happens, we virtually never know them in real life. So what we are going to talk about next is how to \textbf{estimate}
these parameters from data that we observe. The tools we are going to use for estimation come from statistics.

Statistics is a relatively broad term. There are many ways of doing it and chances are that different people from different
fields mean different things when they use the term \textit{statistics}. This is mostly so because the goals that people 
want to achieve using statistics are different. The underlying mechanics do in fact not differ that much. In this course,
we are going to focus on the basics of statistics that you need to know no matter what your goals are. However, allow us
to give you a quick birds-eye view on statistics. 

There are two main goals you can have using statistics (this is grossly oversimplified, but hey, we said it was the birds-eye
view). On the one hand you can do \textbf{descriptive statistics}. This means that you are gathering information about a phenomenon
that you are interested in and report that information. So for example, you might be interested in how many faculty members
at your university are alcoholics. What you do then is you go to each faculty member, check whether they are an alcoholic
and report the total number of alcoholics at your institute. Crucially, you are not going to draw any conclusions (such as that people working in the humanities are more
likely to be alcoholics than science faculty).

Another, somewhat milder examples of descriptive statistics are housing advertisements. If you are looking for a flat, you will usually
find descriptions of the offered flats in terms of square metres, storey, does or does not have a balcony, etc. All of these
can be seen as descriptive statistics. Again, when reviewing these ads, your goal will not be to make a statement like "flats
with a balcony are more habitable than flats without one". This may be a preconception that you have, but it is nothing that
you would be trying to get out of your data.

This brings us to the second big part of statistics, which is \textbf{inferential statistics}. Here you are actually interested in drawing
conclusions (inferences). So if you do an alcoholism survey amongst faculty at your university you would like to find some
way of determining the relationship between area of research, say, and the chance that someone is an alcoholic. The rest of the
course will mostly be about inferential statistics. In particular, our main question will be this: given that we observe some data and
we know (or assume) that the data is distributed according to some distribution (e.g. a multinomial) what are the parameters of that distribution? Hence, we will try to infer the parameters.

Within inferential statistics, there is a further distinction one can make. In \textbf{statistical (data) analysis} people are interested in analysing
the properties of a given data set. Say you have obtained surveys from 500 faculty members. Then your goal would be to make
statements about these 500 surveys. The scope of your study would not extend beyond those 500 data points and all statements that you make
will in principle be limited to this data set. Obviously, if you read a research paper this is not what people do. Researchers
often try to generalize the results they obtain on their data set to a bigger population, like all university employees or even
all of humankind. In the following sections we are going to give some indication for why such generalisations may be justified and
at the same time warn you that they are often not.

Finally, there is the field of \textbf{prediction}. In prediction you again analyse your data, but this time around what you actually want to
do is to predict future data of the same kind. Your current data set is of no actual interest to you except that it allows you
to gather information that may turn out to be valuable for making your predictions. Again, if you have compiled 500 surveys from 
faculty, you would like to predict what the rate of alcoholics for the following 100 surveys is. After you have extracted
the information you need from your original data set, you could even discard it in this setting. In practice, of course, you 
should NEVER discard your data. Instead, you should make it publicly available, so that other people can reproduce your study.
This latter field of prediction is nowadays most commonly known as \textbf{machine learning}. However, statistical analysis and 
prediction are closely intertwined and share a lot of their methodology. It is therefore not always easy to make the distinction.


\section{Statistics and Sample Means}

In the previous section we have introduced the word \textbf{statistic} and also alluded to the fact that we often assume that our
observed data is distributed according to some distribution. The way we usually conceptualize data is that each data point is
an instantiation of a random variable. This means that when you are observing 1000 data points, we conceptualize this as observing
1000 values of random variables. Importantly, each data point could potentially have taken on a different value and it just 
so happens that in our specific \textbf{data sample} it took on the value that it did. 

There is one further assumption that we usually make about our data, namely that it is \textbf{i.i.d.} (identical and independently
distributed). This just means that we assume that all the random variables that generated our data points follow the same distribution and 
that they are independent of each other. When we say they follow the same distribution, we do not just mean the same class of
distributions (e.g. multinomial), but really the same distribution with identical parameters. We often
call that distribution the \textbf{data-generating distribution}, but you also find the terms 
\textit{underlying distribution} or \textit{true distribution} in the literature. 
We have in fact already used the i.i.d.
assumption before. When we do repeated Bernoulli trials, the total probability of the resulting sequence is computed as a product
of independent RVs. We can encode the i.i.d. assumption for $ n $ Bernoulli trials as follows:
\begin{equation}
X_{i} \sim Bernoulli(\theta); 1 \leq i \leq n
\end{equation}

All $ X_{i} $ follow the same distribution since the parameter $ \theta $ does not depend on $ i $ but is constant throughout.
By the same token, we get independence as the distribution does also not depend on other RVs. Thus, repeated Bernoulli trials,
such as repeated coin flips, do actually invoke the i.i.d. assumption. When working with real data this assumption will often be
violated but we are going to make it nonetheless for mathematical convenience or if we can motivate
based on our knowledge of the data set.

After we have described our conception of data, let us move on to defining what a statistic is.

\begin{Definition}
A statistic is the value of any function of a data sample. If we have sampled $ n $ data points that we assume are instantiations of RVs
$ X_{1}^{n} $, a statistic is the value of a function $ g $ on those RVs, i.e. $ g(X_{1}^{n}) $.
\end{Definition}

Arguable the most important statistic in all of statistics is the \textbf{sample mean}. The sample mean is just the average of the
values of the RVs $ X_{1}^{n} $, i.e. of the data points. It is usually denoted by $ \overline{\mu} $. The
sample mean can be seen as guess of the expectation of $ X_{i} $. The expectation is sometimes also
called mean and $ \overline{\mu} $ estimates it from a data sample; hence the name sample mean. Some distributions even have their
mean $ \mu $ as a parameter. To indicate that we are just making a guess at $ \mu $ we put a stroke on top.
This same indicator (or a similar one, like a caret, in which case we would write $ \hat{\mu} $) can obviously be used for other quantities, as well.

\begin{Definition}
The sample mean of i.i.d. random variables $ X_{1}, \ldots, X_{n} $ is defined as 
$$ \overline{\mu} := \dfrac{1}{n}\underset{i=1}{\overset{n}{\sum}} X_{i} \ . $$
\end{Definition}

Notice that since $ \overline{\mu} $ is the average of a collection of random variables, it is itself a random variable. Thus, we can
compute its expectation.
\begin{align}
E_{P_{X}}[\overline{\mu}] &= E_{P_{X}}\left[\dfrac{1}{n} \underset{i=1}{\overset{n}{\sum}} X_{i}\right] \label{eq:sampleMeanBegin} \\
&= \dfrac{1}{n} E_{P_{X}}\left[\underset{i=1}{\overset{n}{\sum}} X_{i}\right] \label{sampleMeanStep1} \\
&= \dfrac{1}{n} \times n E_{P_{X}}[X] \label{sampleMeanStep2} \\
&= E_{P_{X}}[X] = \mu \label{eq:sampleMeanEnd}
\end{align}

This result is huge! Before we interpret it, let us be clear about how we computed it. We take the expectation with respect to
$ P_{X} $. We can do this because all RVs are identically distributed, namely according to $ P_{X} $. Lines \ref{sampleMeanStep1} and
\ref{sampleMeanStep2} follow from the linearity of expectation and the fact that the RVs are i.i.d. 

So why is this result so important? Well, it basically says that if you are getting \textit{enough} data samples and take their sample means,
then the expectation of the sample means will be the true mean underlying your distribution. Since the sample means are distributed according
to some distribution, each sample mean should show up in proportion to its probability. Thus, the mean
of sample means will approximate the true mean. Conceptually this may be quite a bit to chew on, but the practical implications are compelling.
If you are running one experiment (i.e. if you take one data sample) you basically have now clue how probable that sample mean is according
to the distribution of sample means. It could be very improbable and thus not be representative at all of the population you are investigating.
So what should you do? The above result tells us that you should just repeat your experiment \textit{enough} times, so that you get
\textit{enough} sample means. The mean of those sample means will in turn be pretty close to the true mean of the distribution that underlies your population
of interest. This is basically the mathematical reason why in science we want our experimental results to be replicable. If I get a result
and several other people get the same or reasonably close results, we can be fairly sure that we obtained them from a high-probability region
in the distribution of sample means, i.e. the results are indeed representative for the population under scrutiny.

\begin{Exercise}
Under \textbf{some url from course website} you find a file that contains 1000 sequences of 100 numbers each. Those sequences are i.i.d.
samples from a Bernoulli distribution. Write a Python program that computes (an approximation to) the parameter of of that Bernoulli.
\end{Exercise}

The open question at this point is of course how many repetitions are \textit{enough}? We are going to attack this question in Section~\ref{LawOfLargeNumbers}.

\section{Limits}
To help our understanding of the theorem in the next section we are first going to remind ourselves what limits are. For a sequence of numbers
we can ask ourselves whether the sequence will eventually come close to a single point or whether it will just keep moving through the space of real numbers. This
question can be formalized with the concept of limits.

\begin{Definition}[Finite Limit of a sequence]\label{def:finiteLimit}
Take any sequence of real numbers $ \left( a_{n} \right) $ where $ a_{n} : = a(n) $ for some function $ a : \mathbb{N} \rightarrow \mathbb{R} $.
We say that $ L \in \mathbb{R} $ is the limit of that sequence as $ n $ goes to infinity if for any $ \epsilon > 0 $ we can find an 
$ n_{0} \in \mathbb{N} $ such that for any $ n > n_{0} $
$$ |L - a_{n}| < \epsilon\ . $$
We write $ \underset{n \rightarrow \infty}{\lim} a_{n} = L $ to express this fact.
\end{Definition}

\begin{Definition}[Infinite Limit of a sequence]\label{def:infiniteLimit}
Take any sequence of real numbers $ \left( a_{n} \right) $. We say that the sequence diverges (to $ \pm \infty $) if for every $ n_{0},m \in \mathbb{N} $
such that $ |a_{n_{0}}| > m $ there is some $ n_{1} > n_{0} $ such that 
$$ |a_{n_{1}}| > m \ . $$
We write $ \underset{n \rightarrow \infty}{\lim} a_{n} = \pm \infty $ to express this fact.
\end{Definition}

Definition~\ref{def:finiteLimit} tells us that if a sequence converges to a limit $ L $, then for all but finitely many elements (those at the beginning of the sequence)
the difference between each element and $ L $ will be $ \leq \epsilon $. More informally, we can say that the difference between the elements of the sequence and $ L $
can be made arbitrarily small if we are willing to walk far enough down the sequence. 

Definition~\ref{def:infiniteLimit} has been included for completeness' sake but will not be of much relevance in the remainder of the course. Notice, however, that
it is possible that a sequence does not have a limit at all. We will not deal with this case here, though.

\paragraph{Example of a limit calculation}
To give you some more feeling for limits, here is an example. Consider the sequence $ a_{n} = \frac{1}{n} $. What is its limit? Intuitively, $ a_{n} $ becomes smaller
as $ n $ becomes larger. Moreover, all $ a_{n} $ are non-negative. A good guess for the limit thus seems to be $ L = 0 $. Let us show that it is indeed the limit of this
sequence. Choose any real $ \epsilon > 0 $. Then for $ n > n_{0} $ we want that 
\begin{equation} \label{limitexample}
|L - a_{n}| = |a_{n}| = \dfrac{1}{n} < \epsilon\
\end{equation}
We solve this inequality to get $ n > \nicefrac{1}{\epsilon} $. Thus we set $ n_{0} = \nicefrac{1}{\epsilon} $. Since $ \epsilon $ was chosen arbitrarily we conclude
that indeed $ \underset{n \rightarrow \infty}{\lim} \frac{1}{n} = 0 $. $ \square $\bigskip

Instead of limits of sequences, we will actually need limits of functions. However, notice that limits of functions are simply the limits of sequences of function
outputs. 

\begin{Definition}[Limit of a function]
Consider a function $ f $ that is defined on the reals. We say that the limit of $ f(x) $ as $ x $ approaches $ x_{0} $ is $ L $ if for all sequences 
$ \left( a_{n} \right) $ with limit $ x_{0} $ and all $ \epsilon > 0 $ there is an $ n_{0} \in \mathbb{N} $ such that for all $ n > n_{0} $ 
$$  | x_{0} - f\left(a_{n}\right) | < \epsilon \ .$$
We write $ \underset{x \rightarrow x_{0}}{\lim} f(x) = L $ to express this fact. \\
\philip{Actually, this should be something like "A function defined on [a,b] ... all sequences in [a,b]..." but I feel that this would complicate matters unnecessarily.}
\end{Definition}

\section{The Weak Law of Large Numbers}\label{LawOfLargeNumbers}
The weak law of large number states that as we increase our sample size, the probability tends to 0 that our estimated mean $ \overline{\mu} $ will be further than a small
amount $ \epsilon $ away from the true expectation $ E[X] $ of the data-generating distribution $ P_{X} $. In other words, the more sample points we take, 
the smaller is the chance that we will commit a large error when estimating the mean from our sample. To become clear about what we need to prove, let us first state
the weak law of large numbers.

\begin{Theorem}[Weak law of large numbers]\label{weakLaw}
Take i.i.d. distributed random variables $ X_{1}, \ldots, X_{n}, n \in \mathbb{N} $ with distribution $ P_{X} $ and expectation $ \mathbb{E}[X] $. Further let
$ X_{1}^{n} $ have sample mean $ \overline{\mu} = \frac{1}{n} \underset{i=1}{\overset{n}{\sum}X_{i}} $. Then for any real $ \epsilon > 0 $
$$ \underset{n \rightarrow 0}{\lim}P(|\mathbb{E}[X] - \overline{\mu}| < \epsilon) = 0 \ . $$
\end{Theorem}

At this point it may be good to just pause for a moment, stare at the theorem and try to connect it to the verbal explanation from above. The significance of the theorem
derives from the fact that it basically provides us with the theoretical underpinning that allows us to draw inferences.

In order to prove the weak law of large numbers, we use two auxiliary lemmas. Once we have proven those, Theorem~\ref{weakLaw} will follow easily. 

\begin{Lemma}[Markov's inequality]\label{markovIneq}
For any random variable $ X $ and any $ a \in \mathbb{R} $ it holds that
$$ P(X > a) \leq \dfrac{\mathbb{E}[X]}{a} \ . $$
\end{Lemma}

\begin{Exercise}
Proof Markov's inequality.
\end{Exercise}

Besides having established Lemma~\ref{markovIneq}, \href{https://en.wikipedia.org/wiki/Andrey_Markov}{Andrey Markov} has made many other significant contributions to
probability theory. For example, if you go on to take any computational linguistics courses, you are guaranteed to encounter 
\href{https://en.wikipedia.org/wiki/Markov_chain}{Markov chains}. For now, let us move on to our second auxiliary lemma.

\begin{Lemma}[Chebyshev's inequality]\label{Chebyshev}
Let $ X $ be a RV with expectation $ \mathbb{E}[X] $ and variance $ var(X) = \sigma^{2} $. Furthermore, let $ \epsilon > 0 $. Then
$$ Pr(|\mathbb{E}[X] - X| > \epsilon) \leq \dfrac{\sigma}{\epsilon} \ . $$ 
\end{Lemma}

\paragraph{Proof of Lemma~\ref{Chebyshev}} 
First we note that $ Z = |\mathbb{E}[X] - X | $ is itself a random variable. Furthermore, we easily see $ \sigma = \mathbb{E}[Z] $.
So if we plug in $ \mathbb{E}[Z] $ for $ \sigma $, Lemma~\ref{Chebyshev} is proven by Markov's inequality. $ \square $ \bigskip

The final step in proofing the weak law of large numbers is to apply Chebyshev's Inequality in the case where the random variable of interest is
the sample mean.

\paragraph{Proof of Theorem~\ref{weakLaw}} We assume i.i.d. RVs $ X_{1}^{n} $ with expectation $ \mathbb{E}[X] $, and sample mean 
$ \overline{\mu} = \frac{1}{n} \underset{i=1}{\overset{n}{\sum}} X_{i} $, and sample variance 
$ \overline{\sigma}^{2} = \frac{1}{n} \underset{i=1}{\overset{n}{\sum}} \left(\mathbb{E}[X] - X_{i}\right)^{2} $. We note that 
\begin{align}
\mathbb{E}\left[|\mathbb{E}[X] - \overline{\mu}|\right] 
&= \mathbb{E} \left[|\mathbb{E}[X] - \frac{1}{n} \underset{i=1}{\overset{n}{\sum}} X_{i}|\right] \\
&= \dfrac{1}{n} \mathbb{E} \left[|\underset{i=1}{\overset{n}{\sum}}(\mathbb{E}[X] -  X_{i})| \right]
= \dfrac{\overline{\sigma}}{\sqrt{n}} \ .
\end{align}
For the probability $ P(|\mathbb{E}[X] - \overline{\mu}| < \epsilon) $ this mean by Lemma~\ref{Chebyshev} that
\begin{equation}
P(|\mathbb{E}[X] - \overline{\mu}| < \epsilon) < \dfrac{\overline{\sigma}}{\sqrt{n}\epsilon} \ .
\end{equation}
If we fix $ \mathbb{E}[X] $ and $ \epsilon > 0 $ and increase the number of i.i.d. samples, this probability will go to 0.
More formally, choose $ \delta > 0 $. One way to show $ P(|\mathbb{E}[X] - \overline{\mu}| < \epsilon) $ to be smaller than $ \delta $, is to show 
$ \frac{\overline{\sigma}}{\sqrt{n}\epsilon} < \delta $. This implies that $ \left( \frac{\overline{\sigma}}{\delta\epsilon} \right)^{2} < n $. Thus, if we sample
more than $  \left( \frac{\overline{\sigma}}{\delta\epsilon} \right)^{2} $ data points, we can ensure that $ P(|\mathbb{E}[X] - \overline{\mu}| < \epsilon) < \delta $.
Since $ \delta $ is arbitrary, this shows that $ \underset{n \rightarrow \infty}{\lim}~P(|\mathbb{E}[X] - \overline{\mu}| < \epsilon) = 0 $, 
which is exactly what the weak law of large numbers states. $ \square $ \bigskip

The proof of the weak law of large numbers also sheds some new light on the importance of the sample variance. We want our sample variance to be as small as possible
since we will then need fewer samples in order to estimate a close approximation to the true mean of the data-generating distribution. However, the sample variance
is a random variable and hence we can apply the weak law of large numbers to its mean. This means that the more samples we take, the smaller the probability that the
sample variance deviates from the variance of the data-generating distribution by a large margin. Thus, it is helpful if the variance of the 
data-generating distribution be small. Otherwise we will need a lot of samples.

Notice also the relationship between sample variance $ \overline{\sigma}^{2} $ and the number of samples $ n $ in the above proof. For a fixed sample variance, the
probability $ P(|\mathbb{E}[X] - \overline{\mu}| < \epsilon) $ reduces by a factor that is proportional to the square of $ n $. This is to say, that we need to take
more and more samples as we want to decrease the probability that the sample mean deviates from the actual mean by more than $ \epsilon $. 

A more common and more practical interpretation of the relationship between $ \overline{\sigma}^{2} $ and $ n $ is the following: suppose we have collected $ n $ data
points where $ n $ is fixed, meaning we have no quick and cheap way to obtain more data points. Then the lower the sample variance, the more confident we can be
that $ \overline{\mu} $ is a reasonably good approximation to the true mean of the data-generating distribution. This insight also establishes a relationship to
the expectation of sample means (see Equations~\eqref{eq:sampleMeanBegin}-\eqref{eq:sampleMeanEnd}). If the distribution of sample means has low variance, even 
a sample mean with low probability still has good chances of being reasonable close to the true mean.

To visualise the effect of sample size on sample variance, let us take a look at Figure~\ref{fig:likelihood_plots}. The plot on the top shows the same function applied
to different data samples, all of which are of relatively small size. The function plots therefore show large spread as witnessed by the width of the curves. The 
plot at the bottom shows the same function applied to a larger data set. One clearly sees that the spread is much smaller.

\section{Parameter Estimation}

It is time for us to meet \href{https://en.wikipedia.org/wiki/Ronald_Fisher}{Sir Ronald Fisher}, 
one of the founding fathers of statistics. Many of the methods that Fisher introduced
for statistical testing and \textbf{parameter estimation} are still in wide-spread use today. One of his biggest achievements was proposing the 
\textbf{Maximum Likelihood Principle}. To understand this principle, we first have to introduce likelihood functions.

Recall that we can informally write Bayes' Rule as 
\begin{center}
posterior $ \propto $ likelihood $ \times $ prior .
\end{center}
Recall further that every distribution $ P_{X} $ that we have seen so far depends on a number of parameters. 
Once these parameters are set, we can compute the probability of 
any event that is  captured by a value of the RV $ X $. But what can we do if the parameters are not known? It turns out that we can estimate them. In order to 
estimate our parameters, we will make the dependence of $ P_{X} $ on its parameters explicit by letting
\begin{equation}
P(X=x) = P(X=x|\Theta = \theta) \ . 
\end{equation}

This means we regard $ \Theta $ itself as a random variable (over parameters) and use the distribution $ P_{X|\Theta=\theta} $ instead of $ P_{X} $. Notice that for all
$ x $ we have $ P(X=x) = P(X=x|\Theta = \theta) $ as long as the parameters of $ P_{X} $ are set to $ \theta $. Again, the purpose of this substitution is to make
the dependence of the distribution on its parameters explicit.

\begin{Definition}[Likelihood Function]\label{def:likelihood}
For a \emph{fixed} set of data points or observations $ x = x^{1}_{n}, n \in \mathbb{N} $, we define the likelihood function of a 
family of distributions $ P_{x|\Theta} $ as $$ L_{x}(\theta) := P(X=x|\Theta = \theta) \ . $$
\end{Definition}

There are two crucial things to note about the likelihood function. First, the data set $ x $ is assumed to be fixed. Thus, the only random variable that can take
on different values is the parameter RV $ \Theta $. This also tells us that the likelihood function is a function of the parameters \emph{and not of the data}! This is the reason that we index it with the
specific data set $ x $ that we observe and not with a random variable $ X $.

Moreover, the likelihood function is based on conditional probability distributions. If we were to sum over all $ x \in supp(X) $ the result would be one since 
we would be summing over the support of a distribution with parameter vector $ \theta $. Instead, however, the likelihood forces us to leave $ x $ fixed and
only allows us to sum over all values of $ \Theta $. This sum is by no means guaranteed to yield 1 as a result! The important lesson here is that the likelihood 
function is generally \emph{not a probability distribution}! This is a tough pill to swallow in the beginning and you should maybe take a moment to let this sink in
and convince yourself that this is indeed so.

With these (important!) remarks in mind, let us quickly elaborate on notation. Since the data set $ x $ is fixed anyway, many authors do not even bother to include
it as an argument of the likelihood function and just write $ L(\theta) $. Other authors have adopted
the unfortunate convention to write $ L(\theta; x) $. While this is ok when you know what they are talking
about, it may also give you the wrong impression that $ x $ is an argument of the Likelihood function. 

Furthermore, we have made the choice to represent the dependence of the distribution on its
parameters as $ P(X=x|\Theta=\theta) $. This is the Bayesian way of writing this. A frequentist statistician would rather write $ P(X=x; \theta) $ which reads as
``the probability of $ x $ parametrised by $ \theta $''. The crucial difference is that the frequentist would feel uncomfortable to regard the parameters $ \theta $
as a realisation of a random variable because he would claim not to know how to find ``the correct distribution'' $ P_{\Theta} $ for that RV.

The Bayesian statistician, on the other hand, wants to do exactly that: he wants to impose a distribution $ P_{\Theta} $ over the parameters. 
If we look back at Bayes' rule, we see that this distribution would play the part of the prior. If, as in the present case, the prior is a distribution over parameters,
we also call it a parameter prior or prior over parameters. We are siding with the Bayesian view here as it is much easier to interpret and do mathematics with.

After choosing a parameter prior we can compute the posterior distribution over parameters.
\begin{equation}
P(\Theta = \theta|X =x) \propto P(X=x|\Theta = \theta) \times P(\Theta = \theta)
\end{equation}

Notice that we do not compute the distribution $ P(\Theta = \theta|X =x) $ itself but rather a quantity that is proportional to it. It turns out that this will be all
we need in in the remainder of this chapter. Let us emphasize however that this is what makes Bayes' rule so important: it gives us a principled way to compute a
distribution over parameters from data!

With the posterior over parameters at hand, we can now formulate the parameter inference problem. The problem of parameter inference is the problem of picking
a \emph{good} parameter vector. Notice that it is this exact problem that is the referred to as learning
problem when people talk about machine learning. What the machine is trying learn from data are \emph{good} parameters. Where the statistician would talk about
\textbf{parameter estimation}, the computer scientist talks about parameter learning. Both expressions refer to the same thing, but machine learning just sounds a lot
sexier, doesn't it?

We are now left with the question what \emph{good} parameters are. This question does not have a single definitive answer and is actually a constant matter of debate. We
are going to present one classic (and still very relevant) answer which is given by the maximum likelihood principle.


\section{The Maximum Likelihood Principle}

\philip{Need to establish relationship to the posterior distribution here.}

\begin{figure}
\center
\begin{subfigure}{\textwidth}
\center
\includegraphics[scale=.4]{sparse_likelihood.png}
\caption{}
\label{fig:sparse_likelihood}
\end{subfigure}
\begin{subfigure}{\textwidth}
\center
\includegraphics[scale=.4]{dense_likelihood.png}
\caption{}
\label{fig:dense_likelihood} 
\end{subfigure}
\caption{Plots of likelihood functions for different data samples. All data samples were randomly drawn
from a binomial distribution with parameters $ n=10 $ and $ \theta=0.7 $. Figure~\ref{fig:sparse_likelihood}
depicts the likelihood functions for data samples consisting of 2 draws each. 
Figure~\ref{fig:dense_likelihood}
shows the likelihood function for a data sample consisting of 50 draws. The vertical lines indicate
at which value of $ \theta $ the likelihoods reach their maximums.}
\label{fig:likelihood_plots}
\end{figure}

The maximum likelihood principle simply states that we should try to maximize the likelihood function of
our data, that is, we should pick the parameter value that achieves this maximisation. This parameter value
is known as the \textbf{maximum likelihood estimate} (or a maximum likelihood estimates if there are several). 

\begin{Definition}[Maximum Likelihood Estimate]
A maximum likelihood estimate (MLE) for a parameter set $ \Theta $ on a data sample $ x $ 
is a value $ \theta^{*} $ such that
$$ \theta^{*} = \underset{\theta}{arg\,max}\, \, L_{x}(\theta) $$
\end{Definition}

In Figure~\ref{fig:likelihood_plots} we have plotted some likelihood functions. They are based on data
samples that were randomly generated from a binomial distribution with parameters $ n=10 $ and 
$ \theta=0.7 $. The likelihood plots in Figure~\ref{fig:sparse_likelihood} are based on data sets of only
two samples, that is 20 i.i.d. Bernoulli trials. The plot in Figure~\ref{fig:dense_likelihood} is based on a 
data set of 50 samples, that is 500 i.i.d. Bernoulli trials. 
The vertical lines connect the maximum likelihood estimate
for each data set with its likelihood value. Notice the different scales of the two plots. The likelihood value depends on the
data size. Therefore, it does not make sense to compare likelihood values across data sets. After all, each data set comes with its own
likelihood function and the likelihood functions of different data sets are indeed different functions. This is also the
reason why the maximum likelihood principle tells us to only find the parameter value $ \theta $ at which the maximum of the
likelihood function is achieved. There is no point in even looking at the numerical likelihood value of that maximum.

Notice further that in Figure~\ref{fig:likelihood_plots}, there is only one MLE for each of these functions. This is a nice property and we will shortly see why.

Let us now explain how to compute the a MLE. By definition the likelihood function maps the MLE to
one of its maximums. From calculus we know that the derivative of any differentiable function is 0 at the
function's maximums\footnote{Technically we also require that the function be defined on a closed 
(as opposed to open) interval. At this stage it is ok to assume that our parameter sets are always closed
intervals.}. This means that all we need to do in order to find the MLE is to differentiate the likelihood 
function with respect to $ \theta $ and check where the derivative is 0. 
First, however, we need to write down the likelihood function.
Looking back at Definition~\ref{def:likelihood}, we remind ourselves that the likelihood is defined only
with respect to a probabilistic model. In order to write down a likelihood function, we need to first
define such a model. In other words, we will have to define concrete likelihood functions on a case-by-case 
basis. We present one such case as an example below. Before we do so, let us note that for mathematical
convenience, one often uses the logarithm of the likelihood function instead of the likelihood function
itself. This has two advantages: 1) logarithms turn products into sums and sums are often easier to handle
and 2) when using a computer, very small values may be rounded down to 0. Logarithms mitigate this problem
as they turn very small numbers into negative numbers that have rather large absolute values.

\begin{Definition}[Log-Likelihood]
For a \emph{fixed} set of data points or observations $ x = x^{1}_{n}, n \in \mathbb{N} $, we define the 
log-likelihood function of a 
family of distributions $ P_{x|\Theta} $ as 
$$ \mathcal{L}_{x}(\theta) := log \left(L_{x}(\theta) \right) \ . $$
\end{Definition}

Notice the finding a maximum of any the logarithm of any function $ f $ that only yields positive values is the same as finding
the a maximum of the original function $ f $. This is so because the logarithm is strictly monotone, meaning that for any
two possible arguments $ x,y $ of $ f $ we have if $ f(x) > f(y) $ then $ \log(f(x)) > \log(f(y)) $.

In order to make it easier for you to find MLEs of your own, we will give you a procedure that you can apply in most cases and
show you an example of how to use it.
\begin{enumerate}
\item Define a probabilistic model.
\item Write down the functional form of $ L_{x}(\theta) $ for the parameters of that model.
\item Write down $ \mathcal{L}_{x}(\theta) $.
\item Compute $ \frac{\delta}{\delta \theta} \mathcal{L}_{x}(\theta) $.
\item Solve $ 0 = \frac{\delta}{\delta \theta} \mathcal{L}_{x}(\theta) $ for $ \theta $.
\end{enumerate}

\paragraph{Example of finding the MLE} Assume we have to do market research for a clothing store in Amsterdam.
The store wants to expand its size and the manager wonders whether he should use the extra space to 
exhibit more men's or more women's clothing. To help him with this decision, we are going to count 
how many male and female customers are coming in on a given day. We treat the gender of each customer
as a random variable $ X_{i}, 1 \leq i \leq n $ and interpret $ X_{i} = 0 $ as male and $ X_{i} = 1 $ as
female. We assume that the underlying distribution that determines the gender of customers
is the same for all customers. We also assume that a customer's gender is independent the gender of any
other customer. This means we can stipulate that $ X_{i} \bot X_{j} $ whenever $ i \not = j $.

As an aside, notice that these assumptions are not without problems in real applications. 
The first assumption, that the gender
distribution is the same for all customers, may not always be justifiable. Depending on the time of the day
it is possible that more men or women will come in. The second assumption, that the gender of one customer
does not depend on the gender of other customers, may also not always be true. If couples come to shop
at the store, then the gender of one partner will determine the gender of the other partner (in which
way the partners in a couple determine each other's genders depends on whether the couple is homo- or
heterosexual). For the sake of the example we will nevertheless assume that our assumptions hold true.

\textbf{Step 1:} Now as the day is over, we have observed $ k $ women and $ n-k $ men. We postulate a model
according which our model their
occurrences are distributed according a binomial distribution whose parameter $ n $ we already know: it
is simply the total number of our observations (a.k.a. the total number of customers who entered the shop 
that day). What we want in order to facilitate the manager's decision is to estimate $ \theta $, the
probability that a random customer is female. 

\textbf{Step 2:} Our likelihood function looks as follows:
\begin{equation}
L_{x}(\theta) = \binom{n}{k} \theta^{k} \times (1 - \theta)^{n-k} \ .
\end{equation}
As a mnemonic that $ \theta $ is unknown we can informally write
$$ L_{x}(\theta) = \binom{n}{k} ?^{k} \times ?^{n-k} \ . $$

\textbf{Step 3:} We can now take the logarithm of our likelihood function.
\begin{align}
\log(L_{x}(\theta)) &= \log\left(\binom{n}{k} \theta^{k} \times \theta^{n-k} \right) \\
&= \log \left(\binom{n}{k}\right) + k\log(\theta) + (n-k)\log(1-\theta)
\end{align}

\textbf{Step 4:} To find the MLE, we will first differentiate the log-likelihood with respect to $ \theta $.
\begin{align}
\frac{\delta}{\delta \theta} \mathcal{L}_{x}(\theta)
&= \frac{\delta}{\delta \theta} \log \left(\binom{n}{k}\right) + k \frac{\delta}{\delta \theta} \log(\theta) + (n-k) \frac{\delta}{\delta \theta} \log(1-\theta) \\
&= \dfrac{k}{\theta} - \dfrac{n-k}{1-\theta}
\end{align}

\textbf{Step 5:} Finally, we set this derivative to 0 in order to find the maximum of $ \mathcal{L}_{x} $.
\begin{align}
0 &= \dfrac{k}{\theta} - \dfrac{n-k}{1-\theta} &\Leftrightarrow \\
\dfrac{n-k}{1- \theta} &= \dfrac{k}{\theta} &\Leftrightarrow \\
(n-k) \theta &= k (1 - \theta) &\Leftrightarrow \\
k -k\theta &= n\theta - k\theta &\Leftrightarrow \\
\dfrac{k}{k\theta} - 1 &= \dfrac{n\theta}{k\theta} - 1 &\Leftrightarrow \\ 
\dfrac{1}{\theta} &= \dfrac{n}{k} &\Leftrightarrow \\
\theta &= \dfrac{k}{n}
\end{align}

And done we are. You now know once and for all that the MLE for \emph{any} binomial distribution is $ \frac{n}{k} $!

\begin{Exercise}
A coin is getting flipped 1000 times and comes up heads 600 times. According to the MLE, would you say that this coin is fair?
\end{Exercise}

Those of you who are already familiar with calculus may have felt a bit uncomfortable, because we simply state that setting the derivative of the log-likelihood
to 0 will give us a maximum of that function. In general, this technique will only give us an extremum which might as well be a minimum. How can we be so sure that
we really got a maximum? We will just do a prove by picture here and let you do the math. Take another look at Figure~\ref{fig:likelihood_plots}. Clearly, we only
see one maximum and that one is unique in all cases. What you cannot see in the plots is that the likelihood is never 0 for any $ \theta \in (0,1) $. It is just
really, really small, that's why it looks as if it was 0 in many places in the plot. What actually happens is that the likelihood is constantly decreasing as
$ \theta $ approaches 0 and 1. This constant decrease implies that the derivative is not equal to $ 0 $ anywhere other than at the maximum.

Finally, notice that the likelihood is only equal to 0 when $ \theta = 0 $ or $ \theta = 1 $. Thus, the likelihood function does indeed have two minima. However, these 
occur at the boundary points of the interval $ [0,1] $ and since we just stated that $ L_{x}(\theta) $ is constantly decreasing as $ \theta $ approaches those points,
we can conclude that $ \frac{d}{d\theta}L_{x}(0) \not = 0 $ and $ \frac{d}{d\theta}L_{x}(1) = 0 $ at these points. This implies that the only point at which the
derivative of the likelihood function of the binomial distribution is $ 0 $ is at the sole maximum.

We had mentioned above that having only one maximum is a desirable property. This is so because if we want to find the MLE, we have but one choice. If there were
several maxima, we would have to compute all of them and pick amongst them. If there are two maxima that have equal likelihood values, we have no way of choosing
between them and thus no way of determining the single best parameter estimate. Moreover, we will encounter situations in the next chapter where it very hard (if not
impossible) to find the global MLE (i.e. the MLE at the highest maximum).

You may rightfully wonder what distributions have the desirable property of only having one maximum in their likelihood functions. It turns out that those are
exactly the distributions in the \href{https://en.wikipedia.org/wiki/Exponential_family}{exponential family}. The link includes a list of those distributions and
you will be happy to see that most commonly used distributions are members of the exponential family and thus only have one maximum.

\begin{Exercise}
You are given some likelihood function $ L_{x} $ for the binomial distribution along with its MLE $ \theta^{*} $. Show rigorously that $ \theta^{*} $ is indeed
a maximum. That is, show that $ \mathcal{L}_{x}''(\theta^{*}) > 0 $, where $ \mathcal{L}_{x}'' $ is the second derivative of the log-likelihood function.
\end{Exercise}
 
\section{Maximum a Posteriori Estimation}

